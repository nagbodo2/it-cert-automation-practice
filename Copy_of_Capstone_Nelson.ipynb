{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1saBJq/kP3x0YrgZxZNtG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagbodo2/it-cert-automation-practice/blob/master/Copy_of_Capstone_Nelson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on developing an innovative Business Intelligence Assistant using LangChain, Retrieval-Augmented Generation (RAG), and Large Language Models (LLMs) technologies. The tool combines both csv and PDF data and performs comprehensive analysis to identify key trends and patterns. Through RAG, the tool retrieves additional information from Wikipedia to generate insights and recommendations. The tool also allows data visualizations for easier interpretation. The Streamlit User Interface is added for friendly querying the data through prompts. The modules developed in the codes are Data uploading and preparation, Knowledge base creation, LLM application development, data summary, RAG system using Wikipedia corpus, Chain prompts, Memory integration, Model evaluation, Data visualization, and Streamlit User interface. Whenever applicable, we develop a function then apply the function in implementing the modules for easier readability of the code."
      ],
      "metadata": {
        "id": "yiN6ntZhP7gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries\n",
        "!pip install streamlit --quiet\n",
        "!pip install langchain-community chromadb tiktoken pypdf wikipedia matplotlib --quiet"
      ],
      "metadata": {
        "id": "1Drit4v-QQCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### installing required packages\n",
        "import streamlit as st\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import streamlit as st\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.evaluation.qa import QAEvalChain\n",
        "from langchain.schema import Document  # Import Document\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import wikipedia\n",
        "import matplotlib.pyplot as plt\n",
        "import hashlib"
      ],
      "metadata": {
        "id": "k-CY8PJ2EpEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define OpenAI API key once\n",
        "openai_api_key = \" your key\"\n"
      ],
      "metadata": {
        "id": "ONoyDmUCcy7n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to upload the data/documents\n",
        "def load_documents(folder_path):\n",
        "    \"\"\"Loads documents from the specified folder, supporting PDF and CSV files.\"\"\"\n",
        "    db = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith(\".csv\"):\n",
        "            loader = CSVLoader(file_path)\n",
        "        else:\n",
        "            continue  # Skip unsupported file types\n",
        "\n",
        "        loaded_data = loader.load()\n",
        "        db.extend(loaded_data)\n",
        "        print(f\"Loaded {filename}: {len(loaded_data)} documents\")\n",
        "\n",
        "    return db\n"
      ],
      "metadata": {
        "id": "bjfO55Tp3opE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Function to query the sale data\n",
        "def calculate_all_statistics(documents):\n",
        "    \"\"\"\n",
        "    Calculates all possible statistics on all columns, including sum.\n",
        "\n",
        "    Args:\n",
        "        documents: List of Document objects.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing statistics for each column.\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            # Data is comma-separated\n",
        "            content = doc.page_content.strip()  # TO remove leading/trailing spaces\n",
        "            # Split only if content is not empty\n",
        "            if content:\n",
        "                # Replace \\t with , if present\n",
        "                if \"\\t\" in content:\n",
        "                    content = content.replace(\"\\t\", \",\")\n",
        "                # Split content by comma\n",
        "                parts = content.split(\",\")\n",
        "                if len(parts) >= 2:  # Check for at least Product and Sales\n",
        "                    data = {\n",
        "                        \"Product\": parts[0],\n",
        "                        \"Sales\": parts[1],\n",
        "                    }\n",
        "                    all_data.append(data)\n",
        "                else:\n",
        "                    print(f\"Skipping document with invalid format: {content}\")\n",
        "            else:\n",
        "                print(\"Skipping empty document\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error processing document: {e}\")\n",
        "            pass  # Handle errors gracefully\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    # print(df)  # Print the DataFrame to check its structure\n",
        "\n",
        "    # Convert 'Sales' column to numeric, handling errors\n",
        "    df[\"Sales\"] = pd.to_numeric(df[\"Sales\"], errors=\"coerce\")\n",
        "\n",
        "    # Calculate all statistics for all columns, including sum\n",
        "    statistics = {}\n",
        "    for col in df.columns:\n",
        "        col_stats = df[col].describe().to_dict()\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            col_stats[\"sum\"] = df[col].sum()\n",
        "        statistics[col] = col_stats\n",
        "\n",
        "    return statistics, df # Return the dataframe as well\n"
      ],
      "metadata": {
        "id": "16vbCg6JFWmV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "pdf_folder_path = \"/content\"  # Adjust path if necessary\n",
        "documents = load_documents(pdf_folder_path)\n",
        "all_statistics, sales_df = calculate_all_statistics(documents)\n",
        "print(f\"All Statistics:\\n{all_statistics}\")"
      ],
      "metadata": {
        "id": "BR7spcwwRN8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "persist_directory = \"chroma_db\""
      ],
      "metadata": {
        "id": "feViPEToFtub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b6486a-71ec-4ffc-f767-9884cf11f291"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4e170f4acab7>:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Vector Database Management ---\n",
        "statistics_document = [Document(page_content=f\"Sales Statistics:\\n{all_statistics}\",\n",
        "                                 metadata={\"source\": \"sales_statistics\"})]\n"
      ],
      "metadata": {
        "id": "NHTMZokoReee"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch Wikipedia data with error handling, redirects, and rate limiting\n",
        "import time\n",
        "def fetch_wikipedia_data(query, num_results=3, sleep_time=1):\n",
        "    \"\"\"\n",
        "    Fetches relevant Wikipedia pages based on the query.\n",
        "    Handles exceptions, resolves redirects, and adds a delay between requests.\n",
        "    \"\"\"\n",
        "    search_results = wikipedia.search(query, results=num_results)\n",
        "    wikipedia_data = []\n",
        "    for page_title in search_results:\n",
        "        try:\n",
        "            page = wikipedia.page(page_title, auto_suggest=False, redirect=True)\n",
        "            wikipedia_data.append(page.content)\n",
        "            time.sleep(sleep_time)  # Add a delay between requests\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            print(f\"Disambiguation error for {page_title}: {e.options}\")\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            print(f\"Page not found for {page_title}. Trying suggestions...\")\n",
        "            try:\n",
        "                page = wikipedia.page(page_title, auto_suggest=True, redirect=True)\n",
        "                wikipedia_data.append(page.content)\n",
        "                print(f\"Found page using suggestion: {page.title}\")\n",
        "                time.sleep(sleep_time)  # Add a delay after suggestion\n",
        "            except wikipedia.exceptions.PageError:\n",
        "                print(f\"Still could not find page for {page_title} or its suggestions.\")\n",
        "    return wikipedia_data\n",
        "\n",
        "# Fetch Wikipedia data for a relevant topic (e.g., \"Sales\")\n",
        "wikipedia_data = fetch_wikipedia_data(\"Sales\", num_results=3)\n",
        "\n",
        "# Handle empty Wikipedia results\n",
        "if wikipedia_data:\n",
        "    wikipedia_documents = [Document(page_content=content, metadata={\"source\": \"wikipedia\"})\n",
        "                           for content in wikipedia_data]\n",
        "else:\n",
        "    wikipedia_documents = []  # Use an empty list if no Wikipedia data is found\n",
        "    print(\"No relevant Wikipedia data found for the query.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vpv_y0NZR6mb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all documents\n",
        "all_documents = documents + statistics_document + wikipedia_documents"
      ],
      "metadata": {
        "id": "gJPgyzJfe1bq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def hash_document(document):\n",
        "    \"\"\"Hashes a Document object based on its content and metadata.\"\"\"\n",
        "    content_hash = hashlib.md5(document.page_content.encode()).hexdigest()\n",
        "    metadata_hash = hashlib.md5(str(document.metadata).encode()).hexdigest()\n",
        "    combined_hash = hashlib.md5((content_hash + metadata_hash).encode()).hexdigest()\n",
        "    return combined_hash\n",
        "\n",
        "@st.cache_resource(show_spinner=False, hash_funcs={Document: hash_document})\n",
        "def create_vector_database(documents, _embeddings, persist_directory):\n",
        "    \"\"\"Creates a Chroma vector database from documents, embeddings, and a persist directory.\"\"\"\n",
        "    vectordb = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=_embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    return vectordb\n",
        "\n",
        "vectordb = create_vector_database(all_documents, embeddings, persist_directory)"
      ],
      "metadata": {
        "id": "wq7NhuxOVW8E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Memory and Conversational Retrieval Chain ---\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Create chain WITHOUT caching for create_retriever_chain\n",
        "def create_retriever_chain(_llm, _vectordb, memory):\n",
        "    \"\"\"Creates a ConversationalRetrievalChain without caching.\"\"\"\n",
        "    retriever_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=_llm,\n",
        "        retriever=_vectordb.as_retriever(),\n",
        "        memory=memory,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return retriever_chain\n",
        "\n",
        "retriever_chain = create_retriever_chain(llm, vectordb, memory)\n"
      ],
      "metadata": {
        "id": "jWMGTX0uSTJp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edf85d78-0580-4930-f56c-0a8362c27f8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-38bbc23d2aff>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "<ipython-input-14-38bbc23d2aff>:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0, openai_api_key=openai_api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- QAEvalChain Integration ---\n",
        "# Create prediction and eval chains (no caching for eval)\n",
        "prediction_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever())\n",
        "eval_chain = QAEvalChain.from_llm(llm, chain_type=\"stuff\")\n",
        "\n",
        "def query_data(chain, question):\n",
        "    result = chain({\"query\": question})\n",
        "    answer = result[\"result\"]\n",
        "    return answer"
      ],
      "metadata": {
        "id": "WJrGnOIfSbR7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Visualization Functions ---\n",
        "\n",
        "def plot_sales_trends(df):\n",
        "    \"\"\"Plots sales trends over time (assuming a 'Date' column).\"\"\"\n",
        "    # For demonstration, let's assume 'Product' represents time periods\n",
        "    df.groupby('Product')['Sales'].sum().plot(kind='line')\n",
        "    plt.title('Sales Trends Over Time')\n",
        "    plt.xlabel('Product (Time Period)')  # Replace with actual date column if available\n",
        "    plt.ylabel('Sales')\n",
        "    st.pyplot(plt) # Display the plot in Streamlit\n",
        "\n",
        "\n",
        "def plot_product_performance(df):\n",
        "    \"\"\"Plots product performance comparisons.\"\"\"\n",
        "    df.groupby('Product')['Sales'].sum().plot(kind='bar')\n",
        "    plt.title('Product Performance Comparison')\n",
        "    plt.xlabel('Product')\n",
        "    plt.ylabel('Sales')\n",
        "    st.pyplot(plt)  # Display the plot in Streamlit"
      ],
      "metadata": {
        "id": "BjAvxiCcSg-l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Streamlit App ---\n",
        "\n",
        "st.title(\"AI-Powered Business Intelligence Analysis Tool\")\n",
        "\n",
        "# User input for questions\n",
        "user_question = st.text_input(\"Enter your question:\")\n",
        "\n",
        "# Display results and visualizations\n",
        "if user_question:\n",
        "    answer = query_data(retriever_chain, user_question)\n",
        "    st.write(\"**Answer:**\", answer)\n",
        "\n",
        "    # Trigger visualizations based on user question (example)\n",
        "    if \"sales trend\" in user_question.lower():\n",
        "        plot_sales_trends(sales_df)\n",
        "    if \"product performance\" in user_question.lower():\n",
        "        plot_product_performance(sales_df)\n"
      ],
      "metadata": {
        "id": "rVDAowerHG_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljn5_pbtKRh2",
        "outputId": "20a0ed13-ea0f-469b-dfd6-1d7a828cde2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.138.232.33:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Expose port 8501 (default for Streamlit)\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "\n",
        "# Display the public URL in the Streamlit app\n",
        "st.write(f\"[Click here to access the Streamlit app]({public_url})\")"
      ],
      "metadata": {
        "id": "Llpu-5H4JNiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}